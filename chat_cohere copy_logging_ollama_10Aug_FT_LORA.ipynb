{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''%pip install -U langchain-cohere\n",
    "%pip install langchain-huggingface\n",
    "%pip install datasets\n",
    "%pip install transformers huggingface_hub sentence\n",
    "%pip install -U sentence-transformers\n",
    "%pip install sentencepiece\n",
    "%pip install \"transformers[sentencepiece]\"\n",
    "\n",
    "%pip install pdf2image pytesseract'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#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\n",
    "#new token 13 jul 2025\n",
    "# Set your Hugging Face token (keep it secret!)\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"HF_Mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "log_dir = \"data/logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)  # Create logs directory if it doesn't exist\n",
    "log_file = os.path.join(log_dir, f\"hmr_generator_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),  # Write logs to file\n",
    "        logging.StreamHandler()         # Print logs to console\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# Device setup\n",
    "def setup_device():\n",
    "    if torch.backends.cuda.is_built():\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            print(\"Device set to CUDA (NVIDIA GPU)\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"CUDA is available but no GPU detected, falling back to CPU\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CUDA is not built, using CPU\")\n",
    "\n",
    "    return device\n",
    "device = setup_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Documents\n",
    "from langchain.document_loaders import PyPDFLoader,PyPDFDirectoryLoader,DirectoryLoader,TextLoader\n",
    "\n",
    "#dont use pdf part as it will have image,Please use only text pdf \n",
    "'''loader_pdf = DirectoryLoader(\"data/pdf/\" , glob= '**/*.pdf' , show_progress=True ,loader_cls=PyPDFLoader)\n",
    "loader_pdf\n",
    "pdf = loader_pdf.load()\n",
    "len(pdf)'''\n",
    "\n",
    "loader_text = DirectoryLoader(\"data/text/\" , glob= '**/*' , show_progress=True ,loader_cls=TextLoader)\n",
    "text = loader_text.load()\n",
    "print(\"Total numbers of Files\",len(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "\n",
    "path=\"data/pdf\"\n",
    "glob=\"**/*.pdf\"\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path=\"data/pdf\",            # Replace with your actual folder path\n",
    "    glob=\"**/*.pdf\",            # Recursive match for PDFs\n",
    "    loader_cls=PyPDFLoader      # Use PyPDFLoader for each file\n",
    ")\n",
    "\n",
    "import os\n",
    "from langchain.schema import Document\n",
    "from pdf2image import convert_from_path\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "folder_path =\"data/pdf\"\n",
    "def hybrid_pdf_loader(doc):\n",
    "    print(f\"[INFO] Loading: {doc}\")\n",
    "    try:\n",
    "        for doc in folder_path.glob(\"*.pdf\"):\n",
    "            loader = PyPDFLoader(doc)\n",
    "            docs = loader.load()\n",
    "            if all(doc.page_content.strip() == \"\" for doc in docs):\n",
    "                print(f\"[OCR Fallback] Using OCR for: {doc}\")\n",
    "                images = convert_from_path(doc)\n",
    "                ocr_docs = []\n",
    "                for i, image in enumerate(images):\n",
    "                    text = pytesseract.image_to_string(image)\n",
    "                    ocr_docs.append(Document(page_content=text, metadata={\"page\": i + 1, \"source\": doc}))\n",
    "                return ocr_docs\n",
    "            return docs\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed loading {doc}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load all PDFs from folder\n",
    "def load_all_pdfs_from_folder(filepath):\n",
    "    all_docs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            all_docs.extend(hybrid_pdf_loader(full_path))\n",
    "    return all_docs\n",
    "\n",
    "# Usage\n",
    "pdf = load_all_pdfs_from_folder(\"data/pdf\")  # pdf path\n",
    "\n",
    "# Preview output\n",
    "for i, doc in enumerate(pdf[:3]):\n",
    "    print(f\"\\n[Document {i+1} Preview]:\\n{doc.page_content[:300]}\")\n",
    "\n",
    "documents = pdf + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Step 3: Chunking Function\n",
    "def split_documents(documents: list[Document], chunk_size=300, chunk_overlap=50):\n",
    "    # Combine all page_content fields into one string\n",
    "    full_text = \"\\n\".join([doc.page_content for doc in documents if doc.page_content.strip()])\n",
    "\n",
    "    if not full_text.strip():\n",
    "        print(\"[WARNING] No content found in the documents.\")\n",
    "        return []\n",
    "\n",
    "    # Wrap into a single Document object\n",
    "    combined_doc = [Document(page_content=full_text)]\n",
    "\n",
    "    # Initialize the text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\n",
    "            \"\\nheader-rules\\n\",     # HMR block separators\n",
    "            \"\\nelement-rules\\n\",\n",
    "            \"\\nsip-manipulation\\n\",\n",
    "            \"\\n\\n\",\n",
    "            \"\\n\",\n",
    "            \" \",\n",
    "            \"\",\n",
    "            \"\\nmime-sdp-rulesn\",\n",
    "             \"\\nmime-isup-rules\\n\",\n",
    "             \"\\nmime-header-rules\\n\",\n",
    "              \"\\nmime-rules\\n\",\n",
    "              \"\\nisup-param-rules\\n\",\n",
    "              \"\\nsdp-session-rules\\n\",\n",
    "              \"\\nsdp-line-rules\\n\",\n",
    "              \"\\nsdp-media-rules\\n\",\n",
    "\n",
    "        ]\n",
    "        #separators=[\"\\n\\n\", \"\\n\", \" \",\"\"]  # Prioritize bigger splits----old code, not used\n",
    "    )\n",
    "\n",
    "    # Split into chunks\n",
    "    chunks = text_splitter.split_documents(combined_doc)\n",
    "    print(f\"[INFO] Total chunks generated: {len(chunks)}\")\n",
    "\n",
    "    # Preview first 3 chunks(just for check)\n",
    "    for i, chunk in enumerate(chunks[:3]):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\\n{chunk.page_content[:300]}\\n\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Step 4: Run the chunking\n",
    "chunks = split_documents(documents)\n",
    "\n",
    "logger.info(f\"Processed {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Vector Embeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# Use a pre-trained sentence embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2: Ensure chunks are valid Document objects\n",
    "# (Assuming you already have `chunks_val` from your splitter function)\n",
    "print(f\"[INFO] Chunks to embed: {len(chunks)}\")\n",
    "\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "vector_db.persist()\n",
    "\n",
    "\n",
    "#for testing\n",
    "'''def retrieve_relevant_documents(query, k=3):\n",
    "    \"\"\"\n",
    "    Retrieve the top k most relevant document chunks based on the query.\n",
    "    \"\"\"\n",
    "    docs = vector_db.similarity_search(query, k=k)\n",
    "    \n",
    "    retrieved_texts = [doc.page_content for doc in docs]\n",
    "    return retrieved_texts\n",
    "\n",
    "# Example usage:\n",
    "query = \"How to modify the SIP header in Oracle SBC?\"\n",
    "retrieved_docs = retrieve_relevant_documents(query)\n",
    "\n",
    "print(\"Top Retrieved Documents:\")\n",
    "for idx, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {idx+1}:\")\n",
    "    print(doc)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Perfect till abovesteps,Please refer chatgpt for next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMR Generation using Ollama\n",
    "class HMRGenerator:\n",
    "    def __init__(self, vector_db):\n",
    "        self.vector_db = vector_db\n",
    "        self.setup_llm()\n",
    "        self.setup_examples()\n",
    "    \n",
    "    def setup_llm(self):\n",
    "        \"\"\"Initialize Ollama LLM\"\"\"\n",
    "        from langchain.llms import Ollama\n",
    "        try:\n",
    "            self.llm = Ollama(model=\"llama3.2\")  # Make sure this model is installed\n",
    "            logger.info(\"Ollama LLM initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize Ollama: {e}\")\n",
    "            self.llm = None\n",
    "    \n",
    "    def setup_examples(self):\n",
    "        \"\"\"Setup few-shot examples for HMR generation\"\"\"\n",
    "        self.examples = [\n",
    "            {\n",
    "                \"intent\": \"Add P-Asserted-Identity using From header\",\n",
    "                \"sip_msg\": \"INVITE sip:bob@oracle.com SIP/2.0\\nFrom: <sip:alice@telco.com>\",\n",
    "                \"hmr\": \"\"\"header-rules\n",
    "  name                                    addPAI\n",
    "  header-name                             p-asserted-identity\n",
    "  action                                  add\n",
    "  comparison-type                         pattern-rule\n",
    "  msg-type                                request\n",
    "  methods                                 INVITE\n",
    "  new-value                               \"<sip:\"+$From.$From_er.$0+\"@telco.com>\\\"\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"intent\": \"Remove Diversion Header\",\n",
    "                \"sip_msg\": \"INVITE sip:bob@oracle.com SIP/2.0\\nDiversion: <sip:olduser@domain.com>\",\n",
    "                \"hmr\": \"\"\"header-rules\n",
    "  name                                    removeDiversion\n",
    "  header-name                             diversion\n",
    "  action                                  remove\n",
    "  comparison-type                         pattern-rule\n",
    "  msg-type                                request\n",
    "  methods                                 INVITE\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"intent\": \"Replace domain in From header to newdomain.com\",\n",
    "                \"sip_msg\": \"From: <sip:user@olddomain.com>\",\n",
    "                \"hmr\": \"\"\"header-rules\n",
    "  name                                    replaceFromDomain\n",
    "  header-name                             from\n",
    "  action                                  replace\n",
    "  comparison-type                         pattern-rule\n",
    "  msg-type                                request\n",
    "  methods                                 INVITE\n",
    "  match-value                             \"olddomain.com\"\n",
    "  new-value                               \"newdomain.com\\\"\"\"\"\n",
    "            },\n",
    "            {\n",
    "        \"intent\": \"Modify Content-Type in INVITE with SDP\",\n",
    "        \"sip_msg\": \"\",\n",
    "        \"hmr\": \"\"\"header-rules\n",
    "name add_fmtp\n",
    "header-name Content-Type\n",
    "action manipulate\n",
    "comparison-type case-sensitive\n",
    "msg-type any\n",
    "methods INVITE\n",
    "match-value\n",
    "new-value\n",
    "element-rules\n",
    "    name addf\n",
    "    parameter-name application/sdp\n",
    "    type mime\n",
    "    action find-replace-all\n",
    "    match-val-type any\n",
    "    comparison-type pattern-rule\n",
    "    match-value \\Rm=audio.*()[[:1:]]\n",
    "    new-value $CRLF+\"a=fmtp:18 annexb=no\"\n",
    "        \n",
    "\"\"\"\n",
    "    }\n",
    "        ]\n",
    "\n",
    "    def create_prompt(self, intent: str, sip_msg: str, context: str = \"\") -> str:\n",
    "\n",
    "            \"\"\"Create a comprehensive prompt for HMR generation\"\"\"\n",
    "            examples_text = \"\\n\\n\".join([\n",
    "                f\"Example {i+1}:\\nIntent: {ex['intent']}\\nSIP Message:\\n{ex['sip_msg']}\\nHMR:\\n{ex['hmr']}\"\n",
    "                for i, ex in enumerate(self.examples)\n",
    "            ])\n",
    "            \n",
    "            prompt = f\"\"\"You are an expert in Oracle Session Border Controller (SBC) Header Manipulation Rules (HMR).\n",
    "    Your task is to generate accurate HMR CLI configuration based on the user's intent and SIP message.\n",
    "\n",
    "    Context from knowledge base:\n",
    "    {context}\n",
    "\n",
    "    Examples:\n",
    "    {examples_text}\n",
    "\n",
    "    Now generate HMR for:\n",
    "    Intent: {intent}\n",
    "    SIP Message:\n",
    "    {sip_msg}\n",
    "\n",
    "    Generate ONLY the Oracle SBC HMR CLI configuration. Follow the exact format shown in examples.\n",
    "\n",
    "    HMR:\"\"\"\n",
    "            return prompt\n",
    "    \n",
    "    def retrieve_context(self, query: str, k: int = 5) -> str:\n",
    "            \"\"\"Retrieve relevant context from vector database\"\"\"\n",
    "            if not self.vector_db:\n",
    "                return \"\"\n",
    "            \n",
    "            try:\n",
    "                docs = self.vector_db.similarity_search(query, k=k)\n",
    "                context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "                return context\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error retrieving context: {e}\")\n",
    "                return \"\"\n",
    "    \n",
    "    def generate_hmr(self, intent: str, sip_msg: str) -> str:\n",
    "            \"\"\"Generate HMR configuration.intent : str - user intent or description of requirement\n",
    "            sip_msg : str - SIP message content\"\"\"\n",
    "            if not self.llm:\n",
    "                return \"Error: LLM not initialized. Please check Ollama installation.\"\n",
    "            \n",
    "            if not intent.strip():\n",
    "                return \"Error: Please provide an intent/requirement.\"\n",
    "            \n",
    "            try:\n",
    "                # Retrieve relevant context\n",
    "                query = f\"{intent} {sip_msg}\"\n",
    "                context = self.retrieve_context(query)\n",
    "                \n",
    "                # Create prompt\n",
    "                prompt = self.create_prompt(intent, sip_msg, context)\n",
    "                \n",
    "                # Generate response\n",
    "                response = self.llm(prompt)\n",
    "                \n",
    "                # Clean up response\n",
    "                if \"HMR:\" in response:\n",
    "                    hmr_part = response.split(\"HMR:\")[-1].strip()\n",
    "                else:\n",
    "                    hmr_part = response.strip()\n",
    "                \n",
    "                return hmr_part\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating HMR: {e}\")\n",
    "                return f\"Error generating HMR: {str(e)}\"\n",
    "        \n",
    "hmr_generator = HMRGenerator(vector_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hmr_wrapper(intent: str, sip_msg: str) -> str:\n",
    "        \"\"\"Wrapper function for Gradio\"\"\"\n",
    "        if not hmr_generator:\n",
    "            return \"Error: Application not properly initialized\"\n",
    "        hmr_val = hmr_generator.generate_hmr(intent, sip_msg)\n",
    "        if not hmr_val:\n",
    "            return \"Error: No HMR generated. Please check your input.\"\n",
    "        return hmr_val\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"cyan\", secondary_hue=\"gray\")) as demo:\n",
    "    gr.Markdown(\"# Oracle SBC Header Manipulation Rules (HMR) Generator\")\n",
    "    gr.Markdown(\"Generate Oracle SBC HMR CLI configurations based on your requirements and SIP messages.\")\n",
    "    \n",
    "    with gr.Column():\n",
    "        intent_input = gr.Textbox(label=\"Intent / Requirement\", lines=3, placeholder=\"e.g., Add P-Asserted-Identity using From header\")\n",
    "        with gr.Row():\n",
    "            sip_msg_input = gr.Textbox(label=\"SIP Message\", lines=15, placeholder=\"e.g., INVITE sip:bob@oracle.com SIP/2.0\\nFrom: <sip:alice@telco.com>\")\n",
    "            output = gr.Textbox(label=\"Oracle SBC HMR CLI Output\", lines=15, interactive=False,show_copy_button=True)\n",
    "        submit_btn = gr.Button(\"Generate HMR\", variant=\"primary\")\n",
    "\n",
    "        # Bind the function with the llm object\n",
    "        submit_btn.click(\n",
    "            fn=generate_hmr_wrapper,\n",
    "              # Pass llm explicitly\n",
    "            inputs=[intent_input, sip_msg_input],\n",
    "            outputs=output\n",
    "        )\n",
    "        \n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#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\n",
    "# Set your Hugging Face token (keep it secret!)\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"HF_Mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.17.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.12/site-packages (1.3.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/anaconda3/lib/python3.12/site-packages (0.42.0)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/lib/python3.12/site-packages (from peft) (0.5.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /opt/anaconda3/lib/python3.12/site-packages (from peft) (0.32.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (77.0.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Downloading peft-0.17.0-py3-none-any.whl (503 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.9/503.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "Successfully installed peft-0.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# FT dependies\n",
    "# %pip install peft accelerate bitsandbytes datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#RAG all above has good accuracy but below is FT LORA------Testing phase\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# 1. Prepare your HMR training dataset\n",
    "def create_hmr_dataset():\n",
    "    \"\"\"Create training dataset from your existing HMR examples\"\"\"\n",
    "    training_data = []\n",
    "    \n",
    "    # Convert your examples to instruction-response format\n",
    "\n",
    "    examples = [\n",
    "            {\n",
    "                \"intent\": \"Add P-Asserted-Identity using From header\",\n",
    "                \"sip_msg\": \"INVITE sip:bob@oracle.com SIP/2.0\\nFrom: <sip:alice@telco.com>\",\n",
    "                \"hmr\": \"\"\"header-rules\n",
    "  name                                    addPAI\n",
    "  header-name                             p-asserted-identity\n",
    "  action                                  add\n",
    "  comparison-type                         pattern-rule\n",
    "  msg-type                                request\n",
    "  methods                                 INVITE\n",
    "  new-value                               \"<sip:\"+$From.$From_er.$0+\"@telco.com>\\\"\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"intent\": \"Remove Diversion Header\",\n",
    "                \"sip_msg\": \"INVITE sip:bob@oracle.com SIP/2.0\\nDiversion: <sip:olduser@domain.com>\",\n",
    "                \"hmr\": \"\"\"header-rules\n",
    "  name                                    removeDiversion\n",
    "  header-name                             diversion\n",
    "  action                                  remove\n",
    "  comparison-type                         pattern-rule\n",
    "  msg-type                                request\n",
    "  methods                                 INVITE\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"intent\": \"Replace domain in From header to newdomain.com\",\n",
    "                \"sip_msg\": \"From: <sip:user@olddomain.com>\",\n",
    "                \"hmr\": \"\"\"header-rules\n",
    "  name                                    replaceFromDomain\n",
    "  header-name                             from\n",
    "  action                                  replace\n",
    "  comparison-type                         pattern-rule\n",
    "  msg-type                                request\n",
    "  methods                                 INVITE\n",
    "  match-value                             \"olddomain.com\"\n",
    "  new-value                               \"newdomain.com\\\"\"\"\"\n",
    "            },\n",
    "            {\n",
    "        \"intent\": \"Modify Content-Type in INVITE with SDP\",\n",
    "        \"sip_msg\": \"\",\n",
    "        \"hmr\": \"\"\"header-rules\n",
    "name add_fmtp\n",
    "header-name Content-Type\n",
    "action manipulate\n",
    "comparison-type case-sensitive\n",
    "msg-type any\n",
    "methods INVITE\n",
    "match-value\n",
    "new-value\n",
    "element-rules\n",
    "    name addf\n",
    "    parameter-name application/sdp\n",
    "    type mime\n",
    "    action find-replace-all\n",
    "    match-val-type any\n",
    "    comparison-type pattern-rule\n",
    "    match-value \\Rm=audio.*()[[:1:]]\n",
    "    new-value $CRLF+\"a=fmtp:18 annexb=no\"\n",
    "        \n",
    "\"\"\"\n",
    "    }\n",
    "        ]\n",
    "\n",
    "    # Format for training\n",
    "    for example in examples:\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Input:\n",
    "{example['input']}\n",
    "\n",
    "### Response:\n",
    "{example['output']}\"\"\"\n",
    "        training_data.append({\"text\": prompt})\n",
    "    \n",
    "    return Dataset.from_list(training_data)\n",
    "\n",
    "# 2. Setup LoRA configuration\n",
    "def setup_lora_config():\n",
    "    return LoraConfig(\n",
    "        r=16,                    # Rank of adaptation\n",
    "        lora_alpha=32,           # LoRA scaling parameter\n",
    "        target_modules=[         # Target modules for LoRA\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "# 3. Fine-tuning function\n",
    "def fine_tune_llama_for_hmr():\n",
    "    model_name = \"meta-llama/Llama-3.2-3B-Instruct\"  # Use smaller model for local training\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        load_in_4bit=True  # Use 4-bit quantization\n",
    "    )\n",
    "    \n",
    "    # Prepare model for LoRA\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    lora_config = setup_lora_config()\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    dataset = create_hmr_dataset()\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./hmr-llama-lora\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        logging_steps=10,\n",
    "        save_steps=500,\n",
    "        evaluation_strategy=\"no\",\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=False,\n",
    "        report_to=None,  # Disable wandb\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Usage\n",
    "# fine_tuned_model, tokenizer = fine_tune_llama_for_hmr()\n",
    "\n",
    "### Option B: Full Fine-Tuning (Resource Intensive)\n",
    "# For full fine-tuning with more computational resources\n",
    "def full_fine_tune():\n",
    "    # Similar setup but without LoRA\n",
    "    # Requires significant GPU memory (24GB+ recommended)\n",
    "    pass\n",
    "\n",
    "## 2. Fine-Tuning the Retrieval System\n",
    "\n",
    "### Improve Chunking Strategy\n",
    "\n",
    "# Better chunking for HMR-specific content\n",
    "def advanced_chunking(documents):\n",
    "    \"\"\"Improved chunking strategy for HMR documents\"\"\"\n",
    "    \n",
    "    # Custom splitter for HMR configuration blocks\n",
    "    hmr_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,  # Increased for complete HMR blocks\n",
    "        chunk_overlap=100,\n",
    "        separators=[\n",
    "            \"\\nheader-rules\\n\",     # HMR block separators\n",
    "            \"\\nelement-rules\\n\",\n",
    "            \"\\nsip-manipulation\\n\",\n",
    "            \"\\n\\n\",\n",
    "            \"\\n\",\n",
    "            \" \",\n",
    "            \"\",\n",
    "            \"\\nmime-sdp-rulesn\",\n",
    "             \"\\nmime-isup-rules\\n\",\n",
    "             \"\\nmime-header-rules\\n\",\n",
    "              \"\\nmime-rules\\n\",\n",
    "              \"\\nisup-param-rules\\n\",\n",
    "              \"\\nsdp-session-rules\\n\",\n",
    "              \"\\nsdp-line-rules\\n\",\n",
    "              \"\\nsdp-media-rules\\n\",\n",
    "\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return hmr_splitter.split_documents(documents)\n",
    "\n",
    "# Enhanced metadata extraction\n",
    "def extract_hmr_metadata(chunk_text):\n",
    "    \"\"\"Extract structured metadata from HMR chunks\"\"\"\n",
    "    metadata = {}\n",
    "    \n",
    "    if \"header-rules\" in chunk_text:\n",
    "        metadata[\"type\"] = \"header-rule\"\n",
    "    elif \"element-rules\" in chunk_text:\n",
    "        metadata[\"type\"] = \"element-rule\"\n",
    "    elif \"sip-manipulation\" in chunk_text:\n",
    "        metadata[\"type\"] = \"sip-manipulation\"\n",
    "    elif \"mime-sdp-rules\" in chunk_text:\n",
    "        metadata[\"type\"] = \"mime-sdp-rule\"\n",
    "    elif \"mime-isup-rules\" in chunk_text:\n",
    "        metadata[\"type\"] = \"mime-isup-rule\"\n",
    "    elif \"mime-header-rules\" in chunk_text:\n",
    "        metadata[\"type\"] = \"mime-header-rule\"\n",
    "    elif \"mime-rules\" in chunk_text:\n",
    "        metadata[\"type\"] = \"mime-rule\"\n",
    "    elif \"isup-param-rules\" in chunk_text:\n",
    "        metadata[\"type\"] = \"isup-param-rule\"\n",
    "    elif \"sdp-session-rules\" in chunk_text:\n",
    "        metadata[\"type\"] = \"sdp-session-rule\"\n",
    "    elif \"sdp-line-rules\" in chunk_text:\n",
    "        metadata[\"type\"] = \"sdp-line-rule\"\n",
    "    elif \"sdp-media-rules\" in chunk_text:\n",
    "        metadata[\"type\"] = \"sdp-media-rule\"\n",
    "\n",
    "    # Extract action types\n",
    "    actions = [\"add\", \"delete\", \"find-replace-all\", \"manipulate\", \"store\",\"log\",\"none\",\"monitor\",\"reject\",\"sip-manip\",\"store\",\"delete-element\",\"delete-header\"]\n",
    "    for action in actions:\n",
    "        if action in chunk_text.lower():\n",
    "            metadata[\"action\"] = action\n",
    "            break\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "### Optimize Embedding Model\n",
    "\n",
    "# Option 1: Use domain-specific embedding model\n",
    "def setup_better_embeddings():\n",
    "    # Try telecommunications-specific models or train your own\n",
    "    embedding_models = [\n",
    "        \"sentence-transformers/all-MiniLM-L6-v2\",           # Current\n",
    "        \"sentence-transformers/all-mpnet-base-v2\",          # Better quality\n",
    "        \"sentence-transformers/multi-qa-mpnet-base-dot-v1\", # QA-optimized\n",
    "        \"BAAI/bge-small-en-v1.5\"                           # Latest SOTA\n",
    "    ]\n",
    "    \n",
    "    # Benchmark different models\n",
    "    for model_name in embedding_models:\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        # Test retrieval accuracy with your queries\n",
    "        \n",
    "# Option 2: Fine-tune embedding model on your domain\n",
    "def fine_tune_embeddings():\n",
    "    \"\"\"Fine-tune sentence transformer on HMR domain\"\"\"\n",
    "    from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Create training examples from your HMR data\n",
    "    train_examples = []\n",
    "    # Add positive pairs: intent <-> correct HMR\n",
    "    # Add negative pairs: intent <-> incorrect HMR\n",
    "    \n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "    train_loss = losses.CosineSimilarityLoss(model)\n",
    "    \n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=4,\n",
    "        warmup_steps=100,\n",
    "        output_path=\"./hmr-tuned-embeddings\"\n",
    "    )\n",
    "\n",
    "## 3. Prompt Engineering Optimization\n",
    "\n",
    "### Enhanced Prompt Templates\n",
    "\n",
    "def create_advanced_prompt_template():\n",
    "    \"\"\"More sophisticated prompt engineering\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are an Oracle SBC expert specializing in Header Manipulation Rules (HMR). \n",
    "    Your responses must be:\n",
    "    1. Syntactically correct Oracle SBC CLI format\n",
    "    2. Logically sound for the given SIP scenario\n",
    "    3. Following Oracle SBC best practices\n",
    "    4. Include proper error handling when applicable\"\"\"\n",
    "    \n",
    "    few_shot_template = \"\"\"\n",
    "### Context from Knowledge Base:\n",
    "{context}\n",
    "\n",
    "### Examples of Correct HMR Generation:\n",
    "{examples}\n",
    "\n",
    "### Current Task:\n",
    "Intent: {intent}\n",
    "SIP Message: {sip_msg}\n",
    "\n",
    "### Analysis Steps:\n",
    "1. Identify the SIP headers/elements that need modification\n",
    "2. Determine the appropriate HMR action (add/remove/replace/manipulate)\n",
    "3. Consider message type and method restrictions\n",
    "4. Apply proper pattern matching and value substitution\"\"\"\n",
    "\n",
    "### Generate Oracle SBC HMR Configuration:\n",
    "\n",
    "### Dynamic Example Selection\n",
    "\n",
    "def select_relevant_examples(intent, available_examples, k=3):\n",
    "    \"\"\"Dynamically select most relevant examples based on intent\"\"\"\n",
    "\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Encode intent and example intents\n",
    "    intent_embedding = model.encode([intent])\n",
    "    example_embeddings = model.encode([ex[\"intent\"] for ex in available_examples])\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity(intent_embedding, example_embeddings)[0]\n",
    "    \n",
    "    # Select top k most similar examples\n",
    "    top_indices = similarities.argsort()[-k:][::-1]\n",
    "    return [available_examples[i] for i in top_indices]\n",
    "\n",
    "\n",
    "## 4. Retrieval System Optimization\n",
    "\n",
    "### Hybrid Search Strategy\n",
    "\n",
    "def hybrid_retrieval(query, vector_db, k=10):\n",
    "    \"\"\"Combine semantic and keyword-based search\"\"\"\n",
    "    \n",
    "    # Semantic search\n",
    "    semantic_docs = vector_db.similarity_search(query, k=k//2)\n",
    "    \n",
    "    # Keyword search for exact matches\n",
    "    keywords = extract_hmr_keywords(query)\n",
    "    keyword_docs = vector_db.similarity_search(\n",
    "        \" \".join(keywords), \n",
    "        k=k//2,\n",
    "        filter={\"contains_keywords\": True}\n",
    "    )\n",
    "    \n",
    "    # Combine and deduplicate\n",
    "    all_docs = semantic_docs + keyword_docs\n",
    "    unique_docs = remove_duplicates(all_docs)\n",
    "    \n",
    "    return unique_docs[:k]\n",
    "\n",
    "def extract_hmr_keywords(query):\n",
    "    \"\"\"Extract HMR-specific keywords\"\"\"\n",
    "    hmr_keywords = [\n",
    "        \"header-rules\", \"element-rules\", \"sip-manipulation\",\n",
    "        \"add\", \"remove\", \"replace\", \"manipulate\", \"store\",\n",
    "        \"from\", \"to\", \"contact\", \"p-asserted-identity\", \"diversion\"\n",
    "    ]\n",
    "    \n",
    "    found_keywords = []\n",
    "    query_lower = query.lower()\n",
    "    for keyword in hmr_keywords:\n",
    "        if keyword in query_lower:\n",
    "            found_keywords.append(keyword)\n",
    "    \n",
    "    return found_keywords\n",
    "\n",
    "\n",
    "### Re-ranking Retrieved Documents\n",
    "def rerank_documents(query, retrieved_docs):\n",
    "    \"\"\"Re-rank documents based on HMR-specific criteria\"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        score = 0\n",
    "        content = doc.page_content.lower()\n",
    "        \n",
    "        # Boost documents with complete HMR blocks\n",
    "        if \"header-rules\" in content and \"name\" in content:\n",
    "            score += 2.0\n",
    "        \n",
    "        # Boost documents with similar actions\n",
    "        if any(action in query.lower() and action in content \n",
    "               for action in [\"add\", \"remove\", \"replace\", \"modify\"]):\n",
    "            score += 1.5\n",
    "        \n",
    "        # Boost documents with similar headers\n",
    "        headers = [\"from\", \"to\", \"contact\", \"p-asserted-identity\", \"diversion\"]\n",
    "        if any(header in query.lower() and header in content \n",
    "               for header in headers):\n",
    "            score += 1.0\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    # Sort by score\n",
    "    ranked_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), reverse=True)]\n",
    "    return ranked_docs\n",
    "\n",
    "\n",
    "## 5. Training Data Enhancement\n",
    "### Automated Data Augmentation\n",
    "def augment_training_data():\n",
    "    \"\"\"Generate more training examples from existing data\"\"\"\n",
    "    \n",
    "    # Extract patterns from existing HMR files\n",
    "    hmr_patterns = extract_hmr_patterns(\"data/text/\")\n",
    "    \n",
    "    # Generate variations\n",
    "    augmented_examples = []\n",
    "    \n",
    "    for pattern in hmr_patterns:\n",
    "        # Vary header names\n",
    "        headers = [\"from\", \"to\", \"contact\", \"p-asserted-identity\", \"diversion\"]\n",
    "        for header in headers:\n",
    "            if header != pattern[\"original_header\"]:\n",
    "                new_example = create_variation(pattern, new_header=header)\n",
    "                augmented_examples.append(new_example)\n",
    "        \n",
    "        # Vary actions\n",
    "        actions = [\"add\", \"remove\", \"replace\", \"manipulate\"]\n",
    "        for action in actions:\n",
    "            if action != pattern[\"original_action\"]:\n",
    "                new_example = create_variation(pattern, new_action=action)\n",
    "                augmented_examples.append(new_example)\n",
    "    \n",
    "    return augmented_examples\n",
    "\n",
    "def extract_hmr_patterns(data_dir):\n",
    "    \"\"\"Extract reusable patterns from existing HMR files\"\"\"\n",
    "    patterns = []\n",
    "    \n",
    "    for file_path in Path(data_dir).glob(\"*.txt\"):\n",
    "        content = file_path.read_text()\n",
    "        \n",
    "        # Parse HMR blocks\n",
    "        if \"header-rules\" in content:\n",
    "            pattern = parse_hmr_block(content)\n",
    "            patterns.append(pattern)\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "\n",
    "### Validation Dataset Creation\n",
    "\n",
    "def create_validation_dataset():\n",
    "    \"\"\"Create test cases for evaluating HMR generation quality\"\"\"\n",
    "    \n",
    "    validation_cases = [\n",
    "        {\n",
    "            \"intent\": \"Block anonymous calls\",\n",
    "            \"sip_msg\": \"From: \\\"Anonymous\\\" <sip:anonymous@anonymous.invalid>\",\n",
    "            \"expected_hmr_elements\": [\"comparison-type\", \"match-value\", \"anonymous\"]\n",
    "        },\n",
    "        {\n",
    "            \"intent\": \"Add emergency routing\",\n",
    "            \"sip_msg\": \"INVITE sip:911@emergency.com SIP/2.0\",\n",
    "            \"expected_hmr_elements\": [\"911\", \"emergency\", \"add\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return validation_cases\n",
    "\n",
    "## 6. Model Performance Optimization\n",
    "\n",
    "### Quantization for Faster Inference\n",
    "\n",
    "def setup_quantized_model():\n",
    "    \"\"\"Setup 4-bit quantized model for faster inference\"\"\"\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "### Inference Optimization\n",
    "\n",
    "def optimize_generation_params():\n",
    "    \"\"\"Optimized generation parameters for HMR\"\"\"\n",
    "    \n",
    "    generation_config = {\n",
    "        \"max_new_tokens\": 300,\n",
    "        \"temperature\": 0.1,        # Low temperature for consistent output\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 50,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "    \n",
    "    return generation_config\n",
    "\n",
    "\n",
    "## 7. RAG System Fine-Tuning\n",
    "\n",
    "### Improved Context Selection\n",
    "\n",
    "def context_aware_retrieval(intent, sip_msg, vector_db):\n",
    "    \"\"\"Enhanced retrieval with context awareness\"\"\"\n",
    "    \n",
    "    # Multi-query retrieval\n",
    "    queries = [\n",
    "        intent,\n",
    "        f\"SIP {intent}\",\n",
    "        f\"Oracle SBC {intent}\",\n",
    "        extract_sip_method(sip_msg) + \" \" + intent,\n",
    "        extract_headers(sip_msg) + \" manipulation\"\n",
    "    ]\n",
    "    \n",
    "    all_docs = []\n",
    "    for query in queries:\n",
    "        docs = vector_db.similarity_search(query, k=3)\n",
    "        all_docs.extend(docs)\n",
    "    \n",
    "    # Remove duplicates and re-rank\n",
    "    unique_docs = remove_duplicates(all_docs)\n",
    "    reranked_docs = rerank_documents(intent, unique_docs)\n",
    "    \n",
    "    return reranked_docs[:5]  # Top 5 most relevant\n",
    "\n",
    "def extract_sip_method(sip_msg):\n",
    "    \"\"\"Extract SIP method from message\"\"\"\n",
    "    methods = [\"INVITE\", \"REGISTER\", \"BYE\", \"CANCEL\", \"ACK\", \"OPTIONS\"]\n",
    "    for method in methods:\n",
    "        if method in sip_msg:\n",
    "            return method\n",
    "    return \"INVITE\"  # Default\n",
    "\n",
    "def extract_headers(sip_msg):\n",
    "    \"\"\"Extract header names from SIP message\"\"\"\n",
    "    headers = []\n",
    "    lines = sip_msg.split('\\n')\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            header = line.split(':')[0].strip().lower()\n",
    "            headers.append(header)\n",
    "    return \" \".join(headers)\n",
    "\n",
    "\n",
    "## 8. Evaluation and Metrics\n",
    "\n",
    "### HMR Quality Assessment\n",
    "\n",
    "def evaluate_hmr_quality(generated_hmr, expected_elements):\n",
    "    \"\"\"Evaluate generated HMR quality\"\"\"\n",
    "    \n",
    "    metrics = {\n",
    "        \"syntax_score\": check_syntax_correctness(generated_hmr),\n",
    "        \"completeness_score\": check_completeness(generated_hmr, expected_elements),\n",
    "        \"oracle_compliance\": check_oracle_format(generated_hmr),\n",
    "        \"logical_coherence\": check_logical_flow(generated_hmr)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def check_syntax_correctness(hmr):\n",
    "    \"\"\"Check if HMR follows Oracle SBC syntax\"\"\"\n",
    "    required_elements = [\"name\", \"header-name\", \"action\", \"comparison-type\", \"msg-type\"]\n",
    "    score = 0\n",
    "    \n",
    "    for element in required_elements:\n",
    "        if element in hmr:\n",
    "            score += 0.2\n",
    "    \n",
    "    return score\n",
    "\n",
    "def automated_testing_suite():\n",
    "    \"\"\"Automated testing of generated HMRs\"\"\"\n",
    "    \n",
    "    test_cases = load_validation_dataset()\n",
    "    results = []\n",
    "    \n",
    "    for case in test_cases:\n",
    "        generated = hmr_generator.generate_hmr(case[\"intent\"], case[\"sip_msg\"])\n",
    "        metrics = evaluate_hmr_quality(generated, case[\"expected_elements\"])\n",
    "        \n",
    "        results.append({\n",
    "            \"case\": case[\"intent\"],\n",
    "            \"metrics\": metrics,\n",
    "            \"generated_hmr\": generated\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "## 9. Continuous Improvement Pipeline\n",
    "\n",
    "### Feedback Loop Implementation\n",
    "\n",
    "def implement_feedback_loop():\n",
    "    \"\"\"Collect and incorporate user feedback\"\"\"\n",
    "    \n",
    "    # Add to Gradio interface\n",
    "    def gradio_with_feedback():\n",
    "        with gr.Blocks() as demo:\n",
    "            # ... existing interface ...\n",
    "            \n",
    "            with gr.Row():\n",
    "                rating = gr.Slider(1, 5, label=\"Rate HMR Quality\")\n",
    "                feedback = gr.Textbox(label=\"Feedback/Corrections\")\n",
    "                submit_feedback = gr.Button(\"Submit Feedback\")\n",
    "            \n",
    "            def collect_feedback(intent, sip_msg, generated_hmr, rating, feedback):\n",
    "                # Store feedback for retraining\n",
    "                feedback_data = {\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"intent\": intent,\n",
    "                    \"sip_msg\": sip_msg,\n",
    "                    \"generated_hmr\": generated_hmr,\n",
    "                    \"rating\": rating,\n",
    "                    \"feedback\": feedback\n",
    "                }\n",
    "                \n",
    "                # Save to feedback database\n",
    "                save_feedback(feedback_data)\n",
    "                return \"Feedback submitted successfully!\"\n",
    "            \n",
    "            submit_feedback.click(\n",
    "                collect_feedback,\n",
    "                inputs=[intent_input, sip_msg_input, output, rating, feedback],\n",
    "                outputs=gr.Textbox(label=\"Status\")\n",
    "            )\n",
    "\n",
    "\n",
    "### Model Update Pipeline\n",
    "\n",
    "def periodic_model_update():\n",
    "    \"\"\"Periodically retrain with new feedback data\"\"\"\n",
    "    \n",
    "    # 1. Collect feedback data\n",
    "    feedback_data = load_feedback_database()\n",
    "    \n",
    "    # 2. Filter high-quality corrections\n",
    "    good_corrections = [f for f in feedback_data if f[\"rating\"] >= 4]\n",
    "    \n",
    "    # 3. Augment training dataset\n",
    "    new_training_data = convert_feedback_to_training_data(good_corrections)\n",
    "    \n",
    "    # 4. Retrain with LoRA\n",
    "    retrain_model_with_new_data(new_training_data)\n",
    "    \n",
    "    # 5. Evaluate improvements\n",
    "    performance_metrics = evaluate_model_performance()\n",
    "    \n",
    "    return performance_metrics\n",
    "\n",
    "\n",
    "## 10. Advanced Features\n",
    "\n",
    "### Multi-Modal Input Support\n",
    "\n",
    "def support_sip_trace_files():\n",
    "    \"\"\"Support SIP trace file uploads\"\"\"\n",
    "    \n",
    "    def parse_sip_trace(file_content):\n",
    "        \"\"\"Parse Wireshark/tcpdump SIP traces\"\"\"\n",
    "        # Extract SIP messages from trace files\n",
    "        # Identify problematic scenarios\n",
    "        # Suggest appropriate HMRs\n",
    "        pass\n",
    "\n",
    "def intelligent_hmr_chaining():\n",
    "    \"\"\"Generate multiple related HMRs for complex scenarios\"\"\"\n",
    "    \n",
    "    def generate_hmr_sequence(complex_intent):\n",
    "        \"\"\"Break down complex intents into HMR sequence\"\"\"\n",
    "        # Analyze dependencies between HMR rules\n",
    "        # Generate ordered sequence of HMRs\n",
    "        # Validate rule interactions\n",
    "        pass\n",
    "\n",
    "\n",
    "## Implementation Priority\n",
    "\n",
    "### Phase 1: Quick Wins (1-2 weeks)\n",
    "1. Fix the missing `generate_hmr` method\n",
    "2. Optimize chunking strategy for HMR blocks\n",
    "3. Improve prompt engineering with better examples\n",
    "4. Add input validation and error handling\n",
    "\n",
    "### Phase 2: Performance Optimization (2-4 weeks)\n",
    "1. Implement LoRA fine-tuning on your HMR dataset\n",
    "2. Upgrade to better embedding models\n",
    "3. Add evaluation metrics and automated testing\n",
    "4. Implement feedback collection system\n",
    "\n",
    "### Phase 3: Advanced Features (1-2 months)\n",
    "1. Full model fine-tuning if resources allow\n",
    "2. Multi-modal input support\n",
    "3. HMR validation and testing integration\n",
    "4. Continuous learning pipeline\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To begin fine-tuning immediately:\n",
    "\n",
    "1. **Fix Current Issues**: Start with the missing method implementation\n",
    "2. **Data Preparation**: Convert your 124 text files into structured training format\n",
    "3. **LoRA Setup**: Implement the LoRA fine-tuning pipeline above\n",
    "4. **Evaluation**: Create validation metrics for HMR quality\n",
    "5. **Iterate**: Use feedback to continuously improve the system\n",
    "\n",
    "Would you like me to help implement any specific part of this fine-tuning strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 795)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<string>:795\u001b[0;36m\u001b[0m\n\u001b[0;31m    if self.check_ollama_connection():\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gradio as gr\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Langchain imports for RAG\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.schema import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class OllamaHMRRAG:\n",
    "    def __init__(self, model_name=\"llama3.2:3b\", ollama_host=\"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.ollama_host = ollama_host\n",
    "        self.vector_db = None\n",
    "        self.embeddings = None\n",
    "        \n",
    "    def check_ollama_connection(self):\n",
    "        \"\"\"Check if Ollama is running and model is available\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.ollama_host}/api/tags\")\n",
    "            if response.status_code == 200:\n",
    "                models = response.json().get('models', [])\n",
    "                available_models = [model['name'] for model in models]\n",
    "                \n",
    "                if self.model_name not in available_models:\n",
    "                    print(f\"Model {self.model_name} not found. Available models: {available_models}\")\n",
    "                    print(f\"To download the model, run: ollama pull {self.model_name}\")\n",
    "                    return False\n",
    "                    \n",
    "                print(f\"✓ Connected to Ollama. Using model: {self.model_name}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"✗ Cannot connect to Ollama. Make sure it's running.\")\n",
    "                return False\n",
    "                \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"✗ Ollama not running. Start it with: ollama serve\")\n",
    "            return False\n",
    "\n",
    "    def generate_with_ollama(self, prompt: str, temperature: float = 0.1) -> str:\n",
    "        \"\"\"Generate response using Ollama API\"\"\"\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": temperature,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"num_predict\": 500,\n",
    "                \"top_p\": 0.9,\n",
    "                \"repeat_penalty\": 1.1\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(f\"{self.ollama_host}/api/generate\", json=payload)\n",
    "            response.raise_for_status()\n",
    "            return response.json()['response']\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Error calling Ollama: {str(e)}\"\n",
    "\n",
    "    def create_training_examples(self) -> List[Dict]:\n",
    "        \"\"\"Create training examples for fine-tuning context\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"intent\": \"Add P-Asserted-Identity using From header\",\n",
    "                \"sip_msg\": \"INVITE sip:bob@oracle.com SIP/2.0\\nFrom: <sip:alice@telco.com>\",\n",
    "                \"hmr\": \"\"\"header-rules\n",
    "  name                                    addPAI\n",
    "  header-name                             p-asserted-identity\n",
    "  action                                  add\n",
    "  comparison-type                         pattern-rule\n",
    "  msg-type                                request\n",
    "  methods                                 INVITE\n",
    "  new-value                               \"<sip:\"+$From.$From_er.$0+\"@telco.com>\\\"\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"intent\": \"Remove Diversion Header\",\n",
    "                \"sip_msg\": \"INVITE sip:bob@oracle.com SIP/2.0\\nDiversion: <sip:olduser@domain.com>\",\n",
    "                \"hmr\": \"\"\"header-rules\n",
    "  name                                    removeDiversion\n",
    "  header-name                             diversion\n",
    "  action                                  remove\n",
    "  comparison-type                         pattern-rule\n",
    "  msg-type                                request\n",
    "  methods                                 INVITE\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"intent\": \"Replace domain in From header to newdomain.com\",\n",
    "                \"sip_msg\": \"From: <sip:user@olddomain.com>\",\n",
    "                \"hmr\": \"\"\"header-rules\n",
    "  name                                    replaceFromDomain\n",
    "  header-name                             from\n",
    "  action                                  replace\n",
    "  comparison-type                         pattern-rule\n",
    "  msg-type                                request\n",
    "  methods                                 INVITE\n",
    "  match-value                             \"olddomain.com\"\n",
    "  new-value                               \"newdomain.com\\\"\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"intent\": \"Add Contact header with custom domain\",\n",
    "                \"sip_msg\": \"REGISTER sip:example.com SIP/2.0\\nFrom: <sip:user@example.com>\",\n",
    "                \"hmr\": \"\"\"header-rules\n",
    "  name                                    addContact\n",
    "  header-name                             contact\n",
    "  action                                  add\n",
    "  comparison-type                         pattern-rule\n",
    "  msg-type                                request\n",
    "  methods                                 REGISTER\n",
    "  new-value                               \"<sip:user@newdomain.com>\\\"\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"intent\": \"Modify Content-Type for SDP manipulation\",\n",
    "                \"sip_msg\": \"INVITE sip:bob@oracle.com SIP/2.0\\nContent-Type: application/sdp\",\n",
    "                \"hmr\": \"\"\"header-rules\n",
    "  name                                    modify_sdp\n",
    "  header-name                             Content-Type\n",
    "  action                                  manipulate\n",
    "  comparison-type                         case-sensitive\n",
    "  msg-type                                request\n",
    "  methods                                 INVITE\n",
    "  element-rules\n",
    "    name                                  add_fmtp\n",
    "    parameter-name                        application/sdp\n",
    "    type                                  mime\n",
    "    action                                find-replace-all\n",
    "    match-val-type                        any\n",
    "    comparison-type                       pattern-rule\n",
    "    match-value                           m=audio.*\n",
    "    new-value                             $0+$CRLF+\"a=fmtp:18 annexb=no\\\"\"\"\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def advanced_chunking(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Advanced chunking strategy for HMR documents\"\"\"\n",
    "        hmr_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=600,\n",
    "            chunk_overlap=150,\n",
    "            separators=[\n",
    "                \"\\nheader-rules\\n\",\n",
    "                \"\\nelement-rules\\n\", \n",
    "                \"\\nsip-manipulation\\n\",\n",
    "                \"\\nmime-sdp-rules\\n\",\n",
    "                \"\\nmime-isup-rules\\n\",\n",
    "                \"\\nmime-header-rules\\n\",\n",
    "                \"\\nmime-rules\\n\",\n",
    "                \"\\nisup-param-rules\\n\",\n",
    "                \"\\nsdp-session-rules\\n\",\n",
    "                \"\\nsdp-line-rules\\n\",\n",
    "                \"\\nsdp-media-rules\\n\",\n",
    "                \"\\n\\n\",\n",
    "                \"\\n\",\n",
    "                \" \",\n",
    "                \"\"\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return hmr_splitter.split_documents(documents)\n",
    "\n",
    "    def extract_hmr_metadata(self, chunk_text: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract metadata from HMR chunks\"\"\"\n",
    "        metadata = {}\n",
    "        \n",
    "        # Rule type detection\n",
    "        rule_types = {\n",
    "            \"header-rules\": \"header-rule\",\n",
    "            \"element-rules\": \"element-rule\", \n",
    "            \"sip-manipulation\": \"sip-manipulation\",\n",
    "            \"mime-sdp-rules\": \"mime-sdp-rule\",\n",
    "            \"mime-isup-rules\": \"mime-isup-rule\",\n",
    "            \"mime-header-rules\": \"mime-header-rule\",\n",
    "            \"mime-rules\": \"mime-rule\",\n",
    "            \"isup-param-rules\": \"isup-param-rule\",\n",
    "            \"sdp-session-rules\": \"sdp-session-rule\",\n",
    "            \"sdp-line-rules\": \"sdp-line-rule\",\n",
    "            \"sdp-media-rules\": \"sdp-media-rule\"\n",
    "        }\n",
    "        \n",
    "        for rule_pattern, rule_type in rule_types.items():\n",
    "            if rule_pattern in chunk_text:\n",
    "                metadata[\"rule_type\"] = rule_type\n",
    "                break\n",
    "        \n",
    "        # Action detection\n",
    "        actions = [\"add\", \"delete\", \"find-replace-all\", \"manipulate\", \"store\", \n",
    "                  \"log\", \"none\", \"monitor\", \"reject\", \"sip-manip\", \"remove\",\n",
    "                  \"delete-element\", \"delete-header\", \"replace\"]\n",
    "        \n",
    "        for action in actions:\n",
    "            if f\"action {' ' * 10}{action}\" in chunk_text or f\"action{' ' * 20}{action}\" in chunk_text:\n",
    "                metadata[\"action\"] = action\n",
    "                break\n",
    "        \n",
    "        # Header detection\n",
    "        headers = [\"from\", \"to\", \"contact\", \"p-asserted-identity\", \"diversion\", \n",
    "                  \"content-type\", \"via\", \"route\", \"record-route\"]\n",
    "        for header in headers:\n",
    "            if f\"header-name {' ' * 10}{header}\" in chunk_text.lower():\n",
    "                metadata[\"header\"] = header\n",
    "                break\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "    def setup_vector_database(self, data_dir: str = \"data/text/\") -> FAISS:\n",
    "        \"\"\"Setup vector database from HMR documents\"\"\"\n",
    "        print(\"🔍 Setting up vector database...\")\n",
    "        \n",
    "        # Load documents\n",
    "        documents = []\n",
    "        data_path = Path(data_dir)\n",
    "        \n",
    "        if data_path.exists():\n",
    "            for file_path in data_path.glob(\"*.txt\"):\n",
    "                try:\n",
    "                    loader = TextLoader(str(file_path), encoding='utf-8')\n",
    "                    docs = loader.load()\n",
    "                    documents.extend(docs)\n",
    "                    print(f\"✓ Loaded {file_path.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Error loading {file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Directory {data_dir} not found. Creating sample documents...\")\n",
    "        \n",
    "        # Add training examples as documents\n",
    "        training_examples = self.create_training_examples()\n",
    "        for example in training_examples:\n",
    "            doc_content = f\"Intent: {example['intent']}\\nSIP Message: {example['sip_msg']}\\nHMR:\\n{example['hmr']}\"\n",
    "            documents.append(Document(page_content=doc_content, metadata={\"source\": \"training_example\"}))\n",
    "        \n",
    "        if not documents:\n",
    "            print(\"No documents found. Creating minimal sample data...\")\n",
    "            sample_docs = self.create_sample_documents()\n",
    "            documents.extend(sample_docs)\n",
    "        \n",
    "        # Advanced chunking\n",
    "        chunks = self.advanced_chunking(documents)\n",
    "        \n",
    "        # Add metadata\n",
    "        for chunk in chunks:\n",
    "            hmr_metadata = self.extract_hmr_metadata(chunk.page_content)\n",
    "            chunk.metadata.update(hmr_metadata)\n",
    "        \n",
    "        # Create embeddings\n",
    "        print(\"🔗 Creating embeddings...\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "        \n",
    "        # Create vector database\n",
    "        self.vector_db = FAISS.from_documents(chunks, self.embeddings)\n",
    "        \n",
    "        print(f\"✅ Vector database created with {len(chunks)} chunks\")\n",
    "        return self.vector_db\n",
    "\n",
    "    def create_sample_documents(self) -> List[Document]:\n",
    "        \"\"\"Create sample documents for testing\"\"\"\n",
    "        sample_data = [\n",
    "            \"\"\"header-rules\n",
    "  name                                    addFromDomain  \n",
    "  header-name                             from\n",
    "  action                                  add\n",
    "  comparison-type                         pattern-rule\n",
    "  msg-type                                request\n",
    "  methods                                 INVITE\n",
    "  new-value                               \"domain.com\\\"\"\"\",\n",
    "            \n",
    "            \"\"\"header-rules\n",
    "  name                                    removePAI\n",
    "  header-name                             p-asserted-identity  \n",
    "  action                                  remove\n",
    "  comparison-type                         case-sensitive\n",
    "  msg-type                                request\n",
    "  methods                                 INVITE\"\"\",\n",
    "            \n",
    "            \"\"\"element-rules\n",
    "  name                                    modifySDPCodec\n",
    "  type                                    mime\n",
    "  action                                  find-replace-all\n",
    "  match-val-type                          any\n",
    "  comparison-type                         pattern-rule\n",
    "  match-value                             \"m=audio.*\"\n",
    "  new-value                               \"m=audio 5004 RTP/AVP 0 18\\\"\"\"\"\n",
    "        ]\n",
    "        \n",
    "        return [Document(page_content=content, metadata={\"source\": \"sample\"}) for content in sample_data]\n",
    "\n",
    "    def extract_hmr_keywords(self, query: str) -> List[str]:\n",
    "        \"\"\"Extract HMR-specific keywords\"\"\"\n",
    "        hmr_keywords = [\n",
    "            \"header-rules\", \"element-rules\", \"sip-manipulation\",\n",
    "            \"add\", \"remove\", \"replace\", \"manipulate\", \"store\", \"delete\",\n",
    "            \"from\", \"to\", \"contact\", \"p-asserted-identity\", \"diversion\", \"via\",\n",
    "            \"invite\", \"register\", \"bye\", \"cancel\", \"ack\", \"options\",\n",
    "            \"content-type\", \"route\", \"record-route\"\n",
    "        ]\n",
    "        \n",
    "        found_keywords = []\n",
    "        query_lower = query.lower()\n",
    "        for keyword in hmr_keywords:\n",
    "            if keyword in query_lower:\n",
    "                found_keywords.append(keyword)\n",
    "        \n",
    "        return found_keywords\n",
    "\n",
    "    def hybrid_retrieval(self, query: str, k: int = 8) -> List[Document]:\n",
    "        \"\"\"Enhanced retrieval combining semantic and keyword search\"\"\"\n",
    "        if not self.vector_db:\n",
    "            raise ValueError(\"Vector database not initialized. Call setup_vector_database() first.\")\n",
    "        \n",
    "        # Semantic search\n",
    "        semantic_docs = self.vector_db.similarity_search(query, k=k//2)\n",
    "        \n",
    "        # Keyword-enhanced search\n",
    "        keywords = self.extract_hmr_keywords(query)\n",
    "        if keywords:\n",
    "            keyword_query = \" \".join(keywords)\n",
    "            keyword_docs = self.vector_db.similarity_search(keyword_query, k=k//2)\n",
    "        else:\n",
    "            keyword_docs = []\n",
    "        \n",
    "        # Combine and remove duplicates\n",
    "        all_docs = semantic_docs + keyword_docs\n",
    "        unique_docs = self.remove_duplicates(all_docs)\n",
    "        \n",
    "        # Re-rank documents\n",
    "        reranked_docs = self.rerank_documents(query, unique_docs)\n",
    "        \n",
    "        return reranked_docs[:k]\n",
    "\n",
    "    def remove_duplicates(self, docs: List[Document]) -> List[Document]:\n",
    "        \"\"\"Remove duplicate documents\"\"\"\n",
    "        seen = set()\n",
    "        unique_docs = []\n",
    "        \n",
    "        for doc in docs:\n",
    "            content_hash = hash(doc.page_content)\n",
    "            if content_hash not in seen:\n",
    "                seen.add(content_hash)\n",
    "                unique_docs.append(doc)\n",
    "        \n",
    "        return unique_docs\n",
    "\n",
    "    def rerank_documents(self, query: str, docs: List[Document]) -> List[Document]:\n",
    "        \"\"\"Re-rank documents based on HMR-specific criteria\"\"\"\n",
    "        if not docs:\n",
    "            return docs\n",
    "            \n",
    "        scores = []\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        for doc in docs:\n",
    "            score = 1.0  # Base score\n",
    "            content = doc.page_content.lower()\n",
    "            \n",
    "            # Boost complete HMR blocks\n",
    "            if \"header-rules\" in content and \"name\" in content:\n",
    "                score += 3.0\n",
    "            elif \"element-rules\" in content and \"name\" in content:\n",
    "                score += 2.5\n",
    "            \n",
    "            # Boost similar actions\n",
    "            actions = [\"add\", \"remove\", \"replace\", \"modify\", \"manipulate\", \"delete\"]\n",
    "            for action in actions:\n",
    "                if action in query_lower and action in content:\n",
    "                    score += 2.0\n",
    "                    break\n",
    "            \n",
    "            # Boost similar headers\n",
    "            headers = [\"from\", \"to\", \"contact\", \"p-asserted-identity\", \"diversion\", \"via\", \"content-type\"]\n",
    "            for header in headers:\n",
    "                if header in query_lower and header in content:\n",
    "                    score += 1.5\n",
    "                    break\n",
    "            \n",
    "            # Boost SIP methods\n",
    "            methods = [\"invite\", \"register\", \"bye\", \"cancel\", \"options\"]\n",
    "            for method in methods:\n",
    "                if method in query_lower and method in content:\n",
    "                    score += 1.0\n",
    "                    break\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        # Sort by score (descending)\n",
    "        scored_docs = list(zip(scores, docs))\n",
    "        scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        return [doc for score, doc in scored_docs]\n",
    "\n",
    "    def create_hmr_prompt(self, intent: str, sip_msg: str, context_docs: List[Document]) -> str:\n",
    "        \"\"\"Create optimized prompt for HMR generation\"\"\"\n",
    "        \n",
    "        # Prepare context from retrieved documents\n",
    "        context_examples = []\n",
    "        for i, doc in enumerate(context_docs[:3]):  # Use top 3 most relevant\n",
    "            context_examples.append(f\"Example {i+1}:\\n{doc.page_content}\\n\")\n",
    "        \n",
    "        context_str = \"\\n\".join(context_examples) if context_examples else \"No specific examples found.\"\n",
    "        \n",
    "        # Get training examples for few-shot learning\n",
    "        training_examples = self.create_training_examples()\n",
    "        few_shot_examples = []\n",
    "        \n",
    "        # Select most relevant training examples\n",
    "        for example in training_examples[:2]:  # Use 2 best examples\n",
    "            few_shot_examples.append(f\"\"\"\n",
    "Intent: {example['intent']}\n",
    "SIP Message: {example['sip_msg']}\n",
    "Generated HMR:\n",
    "{example['hmr']}\n",
    "\"\"\")\n",
    "        \n",
    "        few_shot_str = \"\\n---\\n\".join(few_shot_examples)\n",
    "        \n",
    "        prompt = f\"\"\"You are an Oracle Session Border Controller (SBC) expert specializing in Header Manipulation Rules (HMR).\n",
    "\n",
    "Your task is to generate syntactically correct Oracle SBC HMR configuration based on the provided intent and SIP message.\n",
    "\n",
    "CONTEXT FROM KNOWLEDGE BASE:\n",
    "{context_str}\n",
    "\n",
    "EXAMPLES OF CORRECT HMR GENERATION:\n",
    "{few_shot_str}\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Generate valid Oracle SBC CLI format\n",
    "2. Use appropriate rule types (header-rules, element-rules, etc.)\n",
    "3. Include all required parameters (name, action, comparison-type, msg-type, etc.)\n",
    "4. Follow Oracle SBC best practices\n",
    "5. Handle the specific SIP scenario correctly\n",
    "\n",
    "CURRENT TASK:\n",
    "Intent: {intent}\n",
    "SIP Message: {sip_msg if sip_msg else \"Not provided\"}\n",
    "\n",
    "ANALYSIS:\n",
    "1. Identify the SIP headers/elements that need modification\n",
    "2. Determine the appropriate HMR action (add/remove/replace/manipulate)\n",
    "3. Consider message type and method restrictions\n",
    "4. Apply proper pattern matching and value substitution\n",
    "\n",
    "Generate the Oracle SBC HMR configuration:\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def generate_hmr(self, intent: str, sip_msg: str = \"\") -> str:\n",
    "        \"\"\"Generate HMR using Ollama with RAG\"\"\"\n",
    "        if not self.check_ollama_connection():\n",
    "            return \"Error: Cannot connect to Ollama. Please ensure it's running and the model is available.\"\n",
    "        \n",
    "        try:\n",
    "            # Retrieve relevant context\n",
    "            query = f\"{intent} {sip_msg}\".strip()\n",
    "            relevant_docs = self.hybrid_retrieval(query, k=5)\n",
    "            \n",
    "            # Create optimized prompt\n",
    "            prompt = self.create_hmr_prompt(intent, sip_msg, relevant_docs)\n",
    "            \n",
    "            # Generate with Ollama\n",
    "            response = self.generate_with_ollama(prompt, temperature=0.1)\n",
    "            \n",
    "            # Clean up the response\n",
    "            cleaned_response = self.clean_hmr_response(response)\n",
    "            \n",
    "            return cleaned_response\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating HMR: {str(e)}\"\n",
    "\n",
    "    def clean_hmr_response(self, response: str) -> str:\n",
    "        \"\"\"Clean and format the HMR response\"\"\"\n",
    "        # Remove any leading/trailing whitespace\n",
    "        response = response.strip()\n",
    "        \n",
    "        # If the response contains the prompt echo, remove it\n",
    "        if \"Generate the Oracle SBC HMR configuration:\" in response:\n",
    "            response = response.split(\"Generate the Oracle SBC HMR configuration:\")[-1].strip()\n",
    "        \n",
    "        # Remove any explanatory text after the HMR block\n",
    "        lines = response.split('\\n')\n",
    "        hmr_lines = []\n",
    "        in_hmr_block = False\n",
    "        \n",
    "        for line in lines:\n",
    "            # Start of HMR block\n",
    "            if any(rule_type in line for rule_type in ['header-rules', 'element-rules', 'sip-manipulation']):\n",
    "                in_hmr_block = True\n",
    "                hmr_lines.append(line)\n",
    "            # Continue HMR block\n",
    "            elif in_hmr_block and (line.strip().startswith(' ') or line.strip() == '' or \n",
    "                                 any(param in line for param in ['name', 'action', 'header-name', 'comparison-type', 'msg-type', 'methods', 'match-value', 'new-value'])):\n",
    "                hmr_lines.append(line)\n",
    "            # End of HMR block\n",
    "            elif in_hmr_block and line.strip() and not line.strip().startswith(' '):\n",
    "                break\n",
    "            # Before HMR block\n",
    "            elif not in_hmr_block:\n",
    "                continue\n",
    "        \n",
    "        return '\\n'.join(hmr_lines) if hmr_lines else response\n",
    "\n",
    "    def evaluate_hmr_quality(self, generated_hmr: str, intent: str = \"\") -> Dict[str, float]:\n",
    "        \"\"\"Evaluate generated HMR quality\"\"\"\n",
    "        metrics = {\n",
    "            \"syntax_score\": self.check_syntax_correctness(generated_hmr),\n",
    "            \"completeness_score\": self.check_completeness(generated_hmr),\n",
    "            \"oracle_compliance\": self.check_oracle_format(generated_hmr),\n",
    "            \"intent_alignment\": self.check_intent_alignment(generated_hmr, intent)\n",
    "        }\n",
    "        \n",
    "        # Overall score\n",
    "        metrics[\"overall_score\"] = sum(metrics.values()) / len(metrics)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def check_syntax_correctness(self, hmr: str) -> float:\n",
    "        \"\"\"Check if HMR follows Oracle SBC syntax\"\"\"\n",
    "        required_elements = [\"name\", \"action\", \"comparison-type\", \"msg-type\"]\n",
    "        optional_elements = [\"header-name\", \"methods\", \"match-value\", \"new-value\"]\n",
    "        \n",
    "        score = 0\n",
    "        hmr_lower = hmr.lower()\n",
    "        \n",
    "        # Check required elements\n",
    "        for element in required_elements:\n",
    "            if element in hmr_lower:\n",
    "                score += 0.2\n",
    "        \n",
    "        # Check for proper indentation (Oracle SBC uses spaces)\n",
    "        lines = hmr.split('\\n')\n",
    "        proper_indentation = sum(1 for line in lines[1:] if line.startswith('  ') or line.strip() == '')\n",
    "        if proper_indentation > 0:\n",
    "            score += 0.1\n",
    "        \n",
    "        # Check for proper rule block start\n",
    "        if any(rule_type in hmr_lower for rule_type in ['header-rules', 'element-rules']):\n",
    "            score += 0.1\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "\n",
    "    def check_completeness(self, hmr: str) -> float:\n",
    "        \"\"\"Check completeness of HMR configuration\"\"\"\n",
    "        score = 0\n",
    "        hmr_lower = hmr.lower()\n",
    "        \n",
    "        # Check for rule name\n",
    "        if \"name\" in hmr_lower:\n",
    "            score += 0.3\n",
    "        \n",
    "        # Check for action\n",
    "        actions = [\"add\", \"remove\", \"replace\", \"manipulate\", \"delete\", \"store\"]\n",
    "        if any(action in hmr_lower for action in actions):\n",
    "            score += 0.3\n",
    "        \n",
    "        # Check for target (header-name or similar)\n",
    "        if any(target in hmr_lower for target in [\"header-name\", \"parameter-name\"]):\n",
    "            score += 0.2\n",
    "        \n",
    "        # Check for message context\n",
    "        if \"msg-type\" in hmr_lower:\n",
    "            score += 0.2\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "\n",
    "    def check_oracle_format(self, hmr: str) -> float:\n",
    "        \"\"\"Check Oracle SBC format compliance\"\"\"\n",
    "        score = 0\n",
    "        \n",
    "        # Check for proper rule block declaration\n",
    "        if any(rule_type in hmr for rule_type in ['header-rules', 'element-rules', 'sip-manipulation']):\n",
    "            score += 0.4\n",
    "        \n",
    "        # Check for proper parameter spacing (Oracle uses specific spacing)\n",
    "        lines = hmr.split('\\n')\n",
    "        proper_format_lines = 0\n",
    "        for line in lines[1:]:  # Skip first line (rule declaration)\n",
    "            if line.strip() and ('  ' in line[:20] or line.strip().startswith('name') or line.strip().startswith('action')):\n",
    "                proper_format_lines += 1\n",
    "        \n",
    "        if proper_format_lines > 0:\n",
    "            score += 0.4\n",
    "        \n",
    "        # Check for reasonable rule name (no special characters, reasonable length)\n",
    "        if \"name\" in hmr.lower():\n",
    "            score += 0.2\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "\n",
    "    def check_intent_alignment(self, hmr: str, intent: str) -> float:\n",
    "        \"\"\"Check if HMR aligns with the stated intent\"\"\"\n",
    "        if not intent:\n",
    "            return 0.5  # Neutral score if no intent provided\n",
    "        \n",
    "        score = 0\n",
    "        intent_lower = intent.lower()\n",
    "        hmr_lower = hmr.lower()\n",
    "        \n",
    "        # Check action alignment\n",
    "        if \"add\" in intent_lower and \"add\" in hmr_lower:\n",
    "            score += 0.3\n",
    "        elif \"remove\" in intent_lower and (\"remove\" in hmr_lower or \"delete\" in hmr_lower):\n",
    "            score += 0.3\n",
    "        elif \"replace\" in intent_lower and \"replace\" in hmr_lower:\n",
    "            score += 0.3\n",
    "        elif \"modify\" in intent_lower and (\"manipulate\" in hmr_lower or \"find-replace\" in hmr_lower):\n",
    "            score += 0.3\n",
    "        \n",
    "        # Check header alignment\n",
    "        headers = [\"from\", \"to\", \"contact\", \"p-asserted-identity\", \"diversion\", \"via\", \"content-type\"]\n",
    "        for header in headers:\n",
    "            if header in intent_lower and header in hmr_lower:\n",
    "                score += 0.4\n",
    "                break\n",
    "        \n",
    "        # Check method alignment\n",
    "        methods = [\"invite\", \"register\", \"bye\", \"cancel\", \"options\",\"message\",\"notify\",\"prack\",\"subscribe\",\"publish\",\"refer\",\"update\"]\n",
    "        for method in methods:\n",
    "            if method in intent_lower and method in hmr_lower:\n",
    "                score += 0.3\n",
    "                break\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "\n",
    "    def create_gradio_interface(self):\n",
    "        \"\"\"Create enhanced Gradio interface\"\"\"\n",
    "        def generate_and_evaluate(intent, sip_msg):\n",
    "            if not intent.strip():\n",
    "                return \"Please provide an intent.\", \"\", \"\"\n",
    "            \n",
    "            try:\n",
    "                # Generate HMR\n",
    "                hmr = self.generate_hmr(intent, sip_msg)\n",
    "                \n",
    "                # Evaluate quality\n",
    "                metrics = self.evaluate_hmr_quality(hmr, intent)\n",
    "                \n",
    "                # Format metrics\n",
    "                metrics_str = f\"\"\"📊 Quality Metrics:\n",
    "• Overall Score: {metrics['overall_score']:.2f}/1.0\n",
    "• Syntax Score: {metrics['syntax_score']:.2f}/1.0\n",
    "• Completeness: {metrics['completeness_score']:.2f}/1.0  \n",
    "• Oracle Compliance: {metrics['oracle_compliance']:.2f}/1.0\n",
    "• Intent Alignment: {metrics['intent_alignment']:.2f}/1.0\n",
    "\n",
    "💡 Score Guide:\n",
    "• 0.8+ : Excellent\n",
    "• 0.6-0.8 : Good  \n",
    "• 0.4-0.6 : Fair\n",
    "• <0.4 : Needs improvement\"\"\"\n",
    "                \n",
    "                # Add recommendations\n",
    "                recommendations = []\n",
    "                if metrics['syntax_score'] < 0.6:\n",
    "                    recommendations.append(\"• Check parameter names and indentation\")\n",
    "                if metrics['completeness_score'] < 0.6:\n",
    "                    recommendations.append(\"• Verify all required parameters are present\")\n",
    "                if metrics['oracle_compliance'] < 0.6:\n",
    "                    recommendations.append(\"• Review Oracle SBC format requirements\")\n",
    "                if metrics['intent_alignment'] < 0.6:\n",
    "                    recommendations.append(\"• Ensure HMR matches the stated intent\")\n",
    "                \n",
    "                if recommendations:\n",
    "                    recommendations_str = \"\\n🔧 Recommendations:\\n\" + \"\\n\".join(recommendations)\n",
    "                else:\n",
    "                    recommendations_str = \"\\n✅ HMR looks good!\"\n",
    "                \n",
    "                return hmr, metrics_str, recommendations_str\n",
    "                \n",
    "            except Exception as e:\n",
    "                return f\"Error: {str(e)}\", \"Error in evaluation\", \"\"\n",
    "                '''\n",
    "        \n",
    "        '''def load_example(example_name):\n",
    "            examples = {\n",
    "                \"Add P-Asserted-Identity\": (\n",
    "                    \"Add P-Asserted-Identity header using From header value\",\n",
    "                    \"INVITE sip:bob@oracle.com SIP/2.0\\nFrom: <sip:alice@telco.com>\"\n",
    "                ),\n",
    "                \"Remove Diversion\": (\n",
    "                    \"Remove Diversion header from INVITE requests\", \n",
    "                    \"INVITE sip:bob@oracle.com SIP/2.0\\nDiversion: <sip:olduser@domain.com>\"\n",
    "                ),\n",
    "                \"Replace Domain\": (\n",
    "                    \"Replace domain in From header to newdomain.com\",\n",
    "                    \"INVITE sip:user@example.com SIP/2.0\\nFrom: <sip:user@olddomain.com>\"\n",
    "                ),\n",
    "                \"Add Contact\": (\n",
    "                    \"Add Contact header for REGISTER requests\",\n",
    "                    \"REGISTER sip:example.com SIP/2.0\\nFrom: <sip:user@example.com>\"\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            if example_name in examples:\n",
    "                return examples[example_name][0], examples[example_name][1]\n",
    "            return \"\", \"\"\n",
    "        '''\n",
    "        \n",
    "        with gr.Blocks(title=\"Oracle SBC HMR Generator\") as interface:\n",
    "            gr.Markdown(\"\"\"\n",
    "            # 🔧 Oracle SBC Header Manipulation Rule Generator\n",
    "            \n",
    "            Generate Oracle Session Border Controller Header Manipulation Rules using AI and RAG technology.\n",
    "            Powered by Ollama and optimized for Oracle SBC configurations.\n",
    "            \"\"\", elem_classes=[\"main-header\"])\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"Input Configuration\")\n",
    "                    \n",
    "                    intent_input = gr.Textbox(\n",
    "                        label=\"Intent\",\n",
    "                        placeholder=\"Describe what you want the HMR to do...\\nExample: Add P-Asserted-Identity header using From header\",\n",
    "                        lines=3,\n",
    "                        info=\"Clearly describe the desired HMR functionality\"\n",
    "                    )\n",
    "                    \n",
    "                    sip_msg_input = gr.Textbox(\n",
    "                        label=\"SIP Message (Optional)\",\n",
    "                        placeholder=\"Provide relevant SIP message content...\\nExample: INVITE sip:bob@oracle.com SIP/2.0\\nFrom: <sip:alice@telco.com>\",\n",
    "                        lines=6,\n",
    "                        info=\"Include relevant SIP headers and message content\"\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        generate_btn = gr.Button(\"🚀 Generate HMR\", variant=\"primary\", size=\"lg\")\n",
    "                        \n",
    "                    \n",
    "                    '''gr.Markdown(\"### 📚 Quick Examples\")\n",
    "                    example_dropdown = gr.Dropdown(\n",
    "                        choices=[\"Add P-Asserted-Identity\", \"Remove Diversion\", \"Replace Domain\", \"Add Contact\"],\n",
    "                        label=\"Load Example\",\n",
    "                        info=\"Select a pre-defined example to get started\"\n",
    "                    )'''\n",
    "                    \n",
    "                    '''# Model Status\n",
    "                    with gr.Accordion(\"🔧 System Status\", open=False):\n",
    "                        status_text = gr.Textbox(\n",
    "                            value=\"Click 'Check Status' to verify Ollama connection\",\n",
    "                            label=\"Ollama Status\",\n",
    "                            interactive=False\n",
    "                        )\n",
    "                        check_status_btn = gr.Button(\"Check Status\")'''\n",
    "\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"### 🎯 Generated HMR Configuration\")\n",
    "                    \n",
    "                    hmr_output = gr.Code(\n",
    "                        label=\"Generated HMR\",\n",
    "                        lines=20,\n",
    "                        show_label=True\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        copy_btn = gr.Button(\"📋 Copy to Clipboard\", size=\"sm\")\n",
    "                        save_btn = gr.Button(\"💾 Save HMR\", size=\"sm\")\n",
    "                    \n",
    "                    '''metrics_output = gr.Textbox(\n",
    "                        label=\"📊 Quality Assessment\",\n",
    "                        lines=12,\n",
    "                        elem_classes=[\"metrics-output\"],\n",
    "                        interactive=False\n",
    "                    )\n",
    "                    \n",
    "                    recommendations_output = gr.Textbox(\n",
    "                        label=\"💡 Recommendations\",\n",
    "                        lines=5,\n",
    "                        interactive=False\n",
    "                    )'''\n",
    "\n",
    "            # Event handlers\n",
    "            generate_btn.click(\n",
    "                generate_and_evaluate,\n",
    "                inputs=[intent_input, sip_msg_input],\n",
    "                outputs=[hmr_output, metrics_output, recommendations_output]\n",
    "            )\n",
    "            \n",
    "            example_dropdown.change(\n",
    "                load_example,\n",
    "                inputs=[example_dropdown],\n",
    "                outputs=[intent_input, sip_msg_input]\n",
    "            )\n",
    "            \n",
    "            '''def check_system_status():\n",
    "                if self.check_ollama_connection():\n",
    "                    return \"✅ Ollama connected successfully! Model ready for use.\"\n",
    "                else:\n",
    "                    return \"❌ Cannot connect to Ollama. Please ensure it's running and model is available.\"\n",
    "            \n",
    "            check_status_btn.click(\n",
    "                check_system_status,\n",
    "                outputs=[status_text]\n",
    "            )\n",
    "            \n",
    "            # Add footer information\n",
    "            gr.Markdown(\"\"\"\n",
    "            ---\n",
    "            ### 📖 How to Use:\n",
    "            1. **Describe your intent** - What should the HMR accomplish?\n",
    "            2. **Provide SIP context** - Include relevant SIP message parts (optional but helpful)\n",
    "            3. **Generate HMR** - Click the generate button to create the configuration\n",
    "            4. **Review quality** - Check the metrics and recommendations\n",
    "            5. **Copy & Deploy** - Use the generated HMR in your Oracle SBC\n",
    "            \n",
    "            ### 🔧 Prerequisites:\n",
    "            - Ollama must be running (`ollama serve`)\n",
    "            - Llama 3.2 model must be available (`ollama pull llama3.2:3b`)\n",
    "            \n",
    "            ### 💡 Tips:\n",
    "            - Be specific in your intent description\n",
    "            - Include relevant SIP headers in the message field\n",
    "            - Use the quality metrics to validate the generated HMR\n",
    "            \"\"\")\n",
    "        '''\n",
    "        return interface\n",
    "\n",
    "    def save_hmr_to_file(self, hmr_content: str, filename: str = None) -> str:\n",
    "        \"\"\"Save generated HMR to file\"\"\"\n",
    "        if not filename:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"generated_hmr_{timestamp}.txt\"\n",
    "        \n",
    "        try:\n",
    "            output_dir = Path(\"generated_hmrs\")\n",
    "            output_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            file_path = output_dir / filename\n",
    "            file_path.write_text(hmr_content)\n",
    "            \n",
    "            return f\"HMR saved to: {file_path}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error saving file: {str(e)}\"\n",
    "\n",
    "    def fine_tune_with_ollama(self, training_data_path: str = None):\n",
    "        \"\"\"\n",
    "        Fine-tune Ollama model with custom HMR data\n",
    "        Note: This requires creating a Modelfile and using ollama create\n",
    "        \"\"\"\n",
    "        print(\"🔧 Setting up Ollama fine-tuning...\")\n",
    "        \n",
    "        # Create training examples in Ollama format\n",
    "        training_examples = self.create_training_examples()\n",
    "        \n",
    "        # Generate Modelfile content\n",
    "        modelfile_content = f\"\"\"FROM {self.model_name}\n",
    "\n",
    "# Set parameters for HMR generation\n",
    "PARAMETER temperature 0.1\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER repeat_penalty 1.1\n",
    "\n",
    "# System message for HMR generation\n",
    "SYSTEM \\\"\\\"\\\"You are an Oracle Session Border Controller (SBC) expert specializing in Header Manipulation Rules (HMR). \n",
    "Generate syntactically correct Oracle SBC CLI format configurations based on user intents and SIP message context.\n",
    "Always follow Oracle SBC best practices and proper parameter formatting.\\\"\\\"\\\"\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Add training examples as few-shot prompts\n",
    "        for i, example in enumerate(training_examples):\n",
    "            modelfile_content += f\"\"\"\n",
    "# Example {i+1}\n",
    "TEMPLATE \\\"\\\"\\\"### Instruction:\n",
    "Generate Oracle SBC Header Manipulation Rule based on the intent and SIP message\n",
    "\n",
    "### Input:\n",
    "Intent: {example['intent']}\n",
    "SIP Message: {example['sip_msg']}\n",
    "\n",
    "### Response:\n",
    "{example['hmr']}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Save Modelfile\n",
    "        modelfile_path = Path(\"Modelfile_HMR\")\n",
    "        modelfile_path.write_text(modelfile_content)\n",
    "        \n",
    "        print(f\"✅ Modelfile created: {modelfile_path}\")\n",
    "        print(\"To create the custom model, run:\")\n",
    "        print(f\"ollama create hmr-specialist -f {modelfile_path}\")\n",
    "        \n",
    "        return str(modelfile_path)\n",
    "\n",
    "    def benchmark_model_performance(self) -> Dict[str, Any]:\n",
    "        \"\"\"Benchmark the model performance on test cases\"\"\"\n",
    "        test_cases = [\n",
    "            {\n",
    "                \"intent\": \"Add P-Asserted-Identity from From header\",\n",
    "                \"sip_msg\": \"INVITE sip:bob@oracle.com SIP/2.0\\nFrom: <sip:alice@telco.com>\",\n",
    "                \"expected_elements\": [\"p-asserted-identity\", \"add\", \"from\"]\n",
    "            },\n",
    "            {\n",
    "                \"intent\": \"Remove Diversion header\",\n",
    "                \"sip_msg\": \"INVITE sip:bob@oracle.com SIP/2.0\\nDiversion: <sip:olduser@domain.com>\",\n",
    "                \"expected_elements\": [\"diversion\", \"remove\"]\n",
    "            },\n",
    "            {\n",
    "                \"intent\": \"Replace domain in Contact header\",\n",
    "                \"sip_msg\": \"REGISTER sip:example.com SIP/2.0\\nContact: <sip:user@olddomain.com>\",\n",
    "                \"expected_elements\": [\"contact\", \"replace\", \"domain\"]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        total_score = 0\n",
    "        \n",
    "        print(\"🧪 Running performance benchmark...\")\n",
    "        \n",
    "        for i, test_case in enumerate(test_cases):\n",
    "            print(f\"Test {i+1}/{len(test_cases)}: {test_case['intent']}\")\n",
    "            \n",
    "            # Generate HMR\n",
    "            generated_hmr = self.generate_hmr(test_case['intent'], test_case['sip_msg'])\n",
    "            \n",
    "            # Evaluate\n",
    "            metrics = self.evaluate_hmr_quality(generated_hmr, test_case['intent'])\n",
    "            \n",
    "            # Check for expected elements\n",
    "            expected_found = sum(1 for element in test_case['expected_elements'] \n",
    "                               if element.lower() in generated_hmr.lower())\n",
    "            expected_score = expected_found / len(test_case['expected_elements'])\n",
    "            \n",
    "            test_result = {\n",
    "                \"test_case\": test_case['intent'],\n",
    "                \"generated_hmr\": generated_hmr,\n",
    "                \"metrics\": metrics,\n",
    "                \"expected_score\": expected_score,\n",
    "                \"overall_score\": (metrics['overall_score'] + expected_score) / 2\n",
    "            }\n",
    "            \n",
    "            results.append(test_result)\n",
    "            total_score += test_result['overall_score']\n",
    "        \n",
    "        benchmark_summary = {\n",
    "            \"individual_results\": results,\n",
    "            \"average_score\": total_score / len(test_cases),\n",
    "            \"total_tests\": len(test_cases),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(f\"📊 Benchmark completed. Average score: {benchmark_summary['average_score']:.2f}\")\n",
    "        \n",
    "        return benchmark_summary\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the HMR RAG system\"\"\"\n",
    "    print(\"🚀 Initializing Oracle SBC HMR Generator with Ollama...\")\n",
    "    \n",
    "    # Initialize the system\n",
    "    hmr_system = OllamaHMRRAG(model_name=\"llama3.2:3b\")\n",
    "    \n",
    "    # Check Ollama connection\n",
    "    if not hmr_system.check_ollama_connection():\n",
    "        print(\"❌ Cannot proceed without Ollama connection.\")\n",
    "        print(\"Please ensure:\")\n",
    "        print(\"1. Ollama is running: ollama serve\")\n",
    "        print(\"2. Model is available: ollama pull llama3.2:3b\")\n",
    "        return\n",
    "    \n",
    "    # Setup vector database\n",
    "    print(\"\\n📚 Setting up knowledge base...\")\n",
    "    try:\n",
    "        hmr_system.setup_vector_database(\"data/text/\")\n",
    "        print(\"✅ Knowledge base ready!\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Warning: Knowledge base setup failed: {e}\")\n",
    "        print(\"Continuing with basic examples...\")\n",
    "        hmr_system.setup_vector_database()\n",
    "    \n",
    "    # Test generation\n",
    "    print(\"\\n🧪 Testing HMR generation...\")\n",
    "    test_intent = \"Add P-Asserted-Identity header using From header\"\n",
    "    test_sip = \"INVITE sip:bob@oracle.com SIP/2.0\\nFrom: <sip:alice@telco.com>\"\n",
    "    \n",
    "    try:\n",
    "        result = hmr_system.generate_hmr(test_intent, test_sip)\n",
    "        print(\"Generated HMR:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(result)\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Evaluate quality\n",
    "        metrics = hmr_system.evaluate_hmr_quality(result, test_intent)\n",
    "        print(f\"\\nQuality Score: {metrics['overall_score']:.2f}/1.0\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test failed: {e}\")\n",
    "    \n",
    "    # Launch Gradio interface\n",
    "    print(\"\\n🌐 Launching web interface...\")\n",
    "    try:\n",
    "        interface = hmr_system.create_gradio_interface()\n",
    "        interface.launch()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to launch interface: {e}\")\n",
    "\n",
    "\n",
    "# Additional utility functions\n",
    "def create_modelfile_for_hmr():\n",
    "    \"\"\"Create a specialized Modelfile for HMR generation\"\"\"\n",
    "    hmr_system = OllamaHMRRAG()\n",
    "    modelfile_path = hmr_system.fine_tune_with_ollama()\n",
    "    print(f\"Modelfile created at: {modelfile_path}\")\n",
    "\n",
    "\n",
    "def run_benchmark():\n",
    "    \"\"\"Run performance benchmark\"\"\"\n",
    "    hmr_system = OllamaHMRRAG()\n",
    "    hmr_system.setup_vector_database()\n",
    "    results = hmr_system.benchmark_model_performance()\n",
    "    \n",
    "    # Save results\n",
    "    import json\n",
    "    with open(\"benchmark_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(\"Benchmark results saved to benchmark_results.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can run different modes:\n",
    "    # main()                    # Full system with GUI\n",
    "    # create_modelfile_for_hmr() # Create Ollama Modelfile\n",
    "    # run_benchmark()           # Run performance tests\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
